{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all the necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as  np\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw dataset is loaded in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data and storing into a dataframe\n",
    "df = pd.read_csv('dataset/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample and total features\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first five rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all the columns in the dataset\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#siplay samples, datatype of each features\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total nan values in each features\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is necessary so understand the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['c-area'].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets rename the columns name for ease use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.rename(columns ={'Suburb' : 'sub',\n",
    "                    'Address' : 'add',\n",
    "                    'Rooms' : 'rooms',\n",
    "                    'Type' : 'type',\n",
    "                    'Price' : 'price',\n",
    "                    'Method' : 'sell-meth',\n",
    "                    'SellerG' : 'seller',\n",
    "                    'Date' : 'date',\n",
    "                    'Distance' : 'dist',\n",
    "                    'PostCode' : 'post-code',\n",
    "                    'Bedroom2' : 'bed2',\n",
    "                    'Bathroom' : 'bathroom',\n",
    "                    'Car' : 'car',\n",
    "                    'Landsize' : 'l-size',\n",
    "                    'BuildingArea' : 'b-area',\n",
    "                    'YearBuilt' : 'build-year',\n",
    "                    'CouncilArea' : 'c-area',\n",
    "                    'Lattitude' : 'lati',\n",
    "                    'Longtitude' : 'long',\n",
    "                    'Regionname' : 'reg-name',\n",
    "                    'Propertycount' : 'prop-count'\n",
    "} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns which are unwanted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(columns= ['date'], inplace= True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Mapping Feature samples for better understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lot samples with in understandable values, so lets map them with appropriate name first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define mapping dictionaries for both columnsasdaf\n",
    "\n",
    "# selling_method_mapping = {\n",
    "#     'SS': 'Sold Before Auction',\n",
    "#     'S': 'Sold',\n",
    "#     'VB': 'Vendor Bid',\n",
    "#     'SP': 'Sold Prior',\n",
    "#     'PI': 'Passed In',\n",
    "#     'SN': 'Sold Not Disclosed',\n",
    "#     'W': 'Withdrawn',\n",
    "#     'PN': 'Passed In, Vendor Bid',\n",
    "#     'SA': 'Sold After Auction'\n",
    "# }\n",
    "# # Use the map function to replace values in both columns\n",
    "# df['sell-meth'] = df['sell-meth'].map(selling_method_mapping)\n",
    "# # Define mapping dictionaries for both columnsasdaf\n",
    "# type_mapping = {'h': 'house', 'u': 'unit', 't': 'town'}\n",
    "# df['type'] = df['type'].map(type_mapping)\n",
    "\n",
    "# # Display the updated DataFrame\n",
    "# df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section covers all the data analysis for single variable exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.0 Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot a histogram of your data\n",
    "plt.hist(data = df, x = 'price', bins=30, density=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.1 Count Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets check all the selling method for each house\n",
    "sns.countplot(data = df, x = 'sell-meth', hue = df['sell-meth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets check the number of rooms in each house\n",
    "sns.countplot(data = df, x = 'rooms', hue = 'rooms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets check the number of rooms in each house\n",
    "sns.countplot(data = df, x = 'type', hue = 'type')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.2 Distribution Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data = df, x = 'price')\n",
    "print('The price is logged.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Multivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.1 Box Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.boxplot(data = df, x= 'type', y = 'price')\n",
    "sns.boxplot(x = df[\"type\"], y = df['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2 Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = df, x ='rooms', y = 'price', hue = 'type') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = df, x ='type', y = 'price', hue = 'type') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = df, x ='dist', y = 'price', hue = 'type') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.3 Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# le = LabelEncoder()\n",
    "# #loading all the categorical features in this\n",
    "# categorical_column = ['sub', 'add','type', 'reg-name', 'sell-meth', 'seller']\n",
    "# for categories in categorical_column:\n",
    "#     df[categories] = le.fit_transform(df[categories])\n",
    "#     le.transform(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.4 Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "#loading all the categorical features in this\n",
    "categorical_column = ['sub', 'add','type', 'reg-name', 'sell-meth', 'seller']\n",
    "for categories in categorical_column:\n",
    "    df[categories] = le.fit_transform(df[categories])\n",
    "    le.transform(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,12))\n",
    "sns.heatmap(df.corr(), annot = True, cmap = 'coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "column_to_test = df['price']\n",
    "\n",
    "# Apply the Shapiro-Wilk test\n",
    "stat, p = stats.shapiro(column_to_test)\n",
    "\n",
    "if p > 0.05:\n",
    "    print(\"The data appears to be normally distributed.\")\n",
    "else:\n",
    "    print(\"The data does not appear to be normally distributed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "kurt = kurtosis(df['price'])\n",
    "skewness = skew(df['price'])\n",
    "\n",
    "if kurt == 3 and skewness == 0:\n",
    "    print(\"The data is normally distributed.\")\n",
    "else:\n",
    "    print(\"The data may not be normally distributed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the correlation matrix seen in 2., we can identify some important features impacting the final price of house. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'] = np.log(df['price'])\n",
    "df['price'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['price'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['rooms', 'type', 'bed2', 'bathroom', 'car', 'long']]\n",
    "y = df[['price']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3, random_state= 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Pre Por\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total nan values in Price\n",
    "null_price = df['price'].isnull().sum()\n",
    "print ('Total number of null values in Price is',null_price,\".\", \"So, this requires some data preprocessing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.dropna(subset=['price'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roomsnull = X_train['rooms'].isnull().sum()\n",
    "bedroomnull= X_train['bed2'].isnull().sum()\n",
    "bathroomnull = X_train['bathroom'].isnull().sum()\n",
    "typenull = X_train['type'].isnull().sum()\n",
    "longnull = X_train['long'].isnull().sum()\n",
    "carnull = X_train['car'].isnull().sum()\n",
    "print ('No. of nulls in is room is : ', roomsnull)\n",
    "# print ('No. of nulls in is price is : ', pricenull)\n",
    "print ('No. of nulls in is bedroom is : ', bedroomnull)\n",
    "print ('No. of nulls in is bathroom is : ',bathroomnull )\n",
    "print ('No. of nulls in is type is : ',typenull)\n",
    "print ('No. of nulls in is longitude is : ',longnull )\n",
    "print ('No. of nulls in is car is : ',carnull )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see a lot of nulls in the dataset. This requires some data processing. \n",
    "Lets check the target for this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['bed2'].fillna(X_train['bed2'].mode()[0], inplace=True)\n",
    "X_train['bathroom'].fillna(X_train['bathroom'].mode()[0], inplace=True)\n",
    "X_train['long'].fillna(X_train['long'].mode()[0], inplace=True)\n",
    "X_train['car'].fillna(X_train['car'].mode()[0], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roomsnull = X_train['rooms'].isnull().sum()\n",
    "bedroomnull= X_train['bed2'].isnull().sum()\n",
    "bathroomnull = X_train['bathroom'].isnull().sum()\n",
    "typenull = X_train['type'].isnull().sum()\n",
    "longnull = X_train['long'].isnull().sum()\n",
    "carnull = X_train['car'].isnull().sum()\n",
    "print ('No. of nulls in is room is : ', roomsnull)\n",
    "# print ('No. of nulls in is price is : ', pricenull)\n",
    "print ('No. of nulls in is bedroom is : ', bedroomnull)\n",
    "print ('No. of nulls in is bathroom is : ',bathroomnull )\n",
    "print ('No. of nulls in is type is : ',typenull)\n",
    "print ('No. of nulls in is longitude is : ',longnull )\n",
    "print ('No. of nulls in is car is : ',carnull )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'price' in y_train.columns and 'price' in y_test.columns:\n",
    "    # Dropping missing values in the 'price' column\n",
    "    y_train['price'].dropna(inplace=True)\n",
    "    y_test['price'].dropna(inplace=True)\n",
    "else:\n",
    "    print(\"The 'price' column does not exist in either y_train or y_test data frames.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.impute import SimpleImputer\n",
    "\n",
    "# # Initialize a SimpleImputer with strategy='median' (you can use other strategies as needed)\n",
    "# imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# # Fit and transform the imputer on the target variable\n",
    "# y_train = imputer.fit_transform(y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 Checking outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a dictionary of columns.\n",
    "# col_dictc = {'type':1\n",
    "           \n",
    "#             }\n",
    "\n",
    "# # Detect outliers in each variable using box plots.\n",
    "# plt.figure(figsize=(20,30))\n",
    "\n",
    "# for variable,i in col_dictc.items():\n",
    "#                      plt.subplot(5,4,i)\n",
    "#                      plt.boxplot(X_train[variable])\n",
    "#                      plt.title(variable)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def outlier_count(col, data = X_train):\n",
    "    \n",
    "#     # calculate your 25% quatile and 75% quatile\n",
    "#     q75, q25 = np.percentile(data[col], [75, 25])\n",
    "    \n",
    "#     # calculate your inter quatile\n",
    "#     iqr = q75 - q25\n",
    "    \n",
    "#     # min_val and max_val\n",
    "#     min_val = q25 - (iqr*1.5)\n",
    "#     max_val = q75 + (iqr*1.5)\n",
    "    \n",
    "#     # count number of outliers, which are the data that are less than min_val or more than max_val calculated above\n",
    "#     outlier_count = len(np.where((data[col] > max_val) | (data[col] < min_val))[0])\n",
    "    \n",
    "#     # calculate the percentage of the outliers\n",
    "#     outlier_percent = round(outlier_count/len(data[col])*100, 2)\n",
    "    \n",
    "#     if(outlier_count > 0):\n",
    "#         print(\"\\n\"+15*'-' + col + 15*'-'+\"\\n\")\n",
    "#         print('Number of outliers: {}'.format(outlier_count))\n",
    "#         print('Percent of data that is outlier: {}%'.format(outlier_percent))\n",
    "# for col in X_train.columns:\n",
    "#     outlier_count(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3 Scalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape check for X_train, X_test, y_train, y_test before model fitting\n",
    "print(\"Shape of X_train: \", X_train.shape)\n",
    "print(\"Shape of X_test: \", X_test.shape)\n",
    "print(\"Shape of y_train: \", y_train.shape)\n",
    "print(\"Shape of y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the algorithm libaries to be tried out\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Listing out the algorithms to be tried out\n",
    "algorithms = [\n",
    "    LinearRegression(),\n",
    "    SVR(),\n",
    "    KNeighborsRegressor(),\n",
    "    DecisionTreeRegressor(random_state = 0),\n",
    "    RandomForestRegressor(n_estimators = 200, random_state = 42),\n",
    "    XGBRegressor(n_estimators = 200, random_state = 42)\n",
    "]\n",
    "\n",
    "algorithm_names = [\n",
    "    \"Linear Regression\",\n",
    "    \"SVR\",\n",
    "    \"KNeighbors Regressor\",\n",
    "    \"Decision-Tree Regressor\",\n",
    "    \"Random-Forest Regressor\",\n",
    "    \"XGBregressor\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.isnull().sum())\n",
    "print(y_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train.dropna(subset=['price'], inplace=True)\n",
    "# y_test.dropna(subset=['price'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.isnull().sum())\n",
    "print(y_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library KFold and cross_val_score\n",
    "# These libaries will cross validate the best scores between algorithms and result mean of best score for each algorith\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "train_mse = []\n",
    "test_mse = []\n",
    "\n",
    "# KFold split is set to 10, hence the mean of ten scores will be taken\n",
    "kfold = KFold(n_splits = 10, shuffle=True)\n",
    "\n",
    "# Looping each algorithm for cross validation using training data\n",
    "# The scoring is set to best negative mean squared error\n",
    "for i, model in enumerate(algorithms):\n",
    "    scores = cross_val_score(\n",
    "        model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=kfold,\n",
    "        scoring='neg_mean_squared_error'\n",
    "    )\n",
    "    \n",
    "    print(f\"{algorithm_names[i]} - Score: {scores}; Mean: {scores.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = float('-inf')  # Initialize with negative infinity\n",
    "mean_score = scores.mean()\n",
    "print(f\"{algorithm_names[i]} - Score: {scores}; Mean: {mean_score}\")\n",
    "    \n",
    "if mean_score > best_score:\n",
    "        best_score = mean_score\n",
    "        best_model_index = i\n",
    "\n",
    "best_model = algorithms[best_model_index]\n",
    "print(f\"The best model is {algorithm_names[best_model_index]} with a mean score of {best_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Grid Search Library\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "# Grid Search takes in an algorithm and applies different values to hyperparaemeters and computes resulst for each permutation of parameter values\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Defining the values for parameters\n",
    "param_grid = {\n",
    "    'max_depth': [5, 10, None],\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.1]\n",
    "}\n",
    "\n",
    "xgb = XGBRegressor(random_state = 42)\n",
    "\n",
    "# Initalize grid for XBGRegressor algorithm with above parameters values\n",
    "# The scoring is to best negative mean squared error\n",
    "grid = GridSearchCV(\n",
    "    estimator = xgb,\n",
    "    param_grid = param_grid,\n",
    "    cv = kFold,\n",
    "    n_jobs = -1,\n",
    "    return_train_score=True,\n",
    "    refit=True,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "# The training set is fitted to the above grid\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the best parameters values while training the model\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the best mse value from the grid\n",
    "# The best mse is result of applying the best paramater values in XGBRegressor algorithm\n",
    "best_mse = grid.best_score_\n",
    "best_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "yhat = grid.predict(X_test)\n",
    "\n",
    "mean_squared_error(yhat, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgb_best_estimator = grid.best_estimator_\n",
    "\n",
    "# Extracting the feature importance scores from the grid\n",
    "xgb_best_estimator.feature_importances_\n",
    "\n",
    "# Bar plot for the features and thier importance\n",
    "plt.barh(X.columns, xgb_best_estimator.feature_importances_)\n",
    "plt.xlabel(\"XGB Regressor Feature Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pickle library\n",
    "import pickle\n",
    "\n",
    "# Exporting the model to selling-price.model\n",
    "filename = '../model/selling-price.model'\n",
    "pickle.dump(grid, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will also dump the scaler values for future use\n",
    "scaler_filename = '../model/scaler.pkl'\n",
    "pickle.dump(scaler, open(scaler_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the model\n",
    "selling_price_model = pickle.load(open(\"../model/selling-price.model\", \"rb\"))\n",
    "\n",
    "# Creating a dummy sample\n",
    "sample = {\n",
    "    \"max_power\": [100],\n",
    "    \"engine\": [1200],\n",
    "    \"mileage\": [23]\n",
    "}\n",
    "\n",
    "# Convert the sample to panda dataframe\n",
    "sample = pd.DataFrame(sample)\n",
    "\n",
    "# Scale the sample using the same scaler used for X_train and X_set\n",
    "scaled_sample = scaler.transform(sample)\n",
    "\n",
    "# Use the model to predict the selling price\n",
    "predicted_selling_price = selling_price_model.predict(scaled_sample)\n",
    "\n",
    "# As the we have log transformed the y while training and set, we will need to exponent transform the predicted value for correct prediction\n",
    "predicted_selling_price = np.exp(predicted_selling_price)\n",
    "\n",
    "print(\"The predicted selling price is \" + str(predicted_selling_price[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
